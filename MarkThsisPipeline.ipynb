{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MarkThsisPipeline.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IU4p6KZ-RyVd"
      },
      "source": [
        "#Natural Language Processing for the Chinese Language -- Text Analysis of News Articles for Stock Price Prediction\n",
        "End result:\n",
        "1. Calculated sentiments for titles, article previews, and texts.\n",
        "2. Used part-of-speech tagging to extract grammaticle components in a sentence, e.g. noun phrases, verbs etc, and ranked them based on frequency. \n",
        "\n",
        "Package Used \n",
        "*   SnowNLP\n",
        "*   baiduAip\n",
        "\n",
        "\n",
        "3. Used pointwise mutual information (algorithm) to rank words based on their positivity/negativity and frequency. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2EVY5NWEgSm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5767d38b-92f9-4aef-fa31-d4e6e28797c7"
      },
      "source": [
        "!pip install snownlp==0.12.3\n",
        "!pip install nltk\n",
        "!pip install stopwords-zh\n",
        "!pip install baidu-aip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting snownlp==0.12.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/b3/37567686662100d3bce62d3b0f2adec18ab4b9ff2b61abd7a61c39343c1d/snownlp-0.12.3.tar.gz (37.6MB)\n",
            "\u001b[K     |████████████████████████████████| 37.6MB 119kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: snownlp\n",
            "  Building wheel for snownlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for snownlp: filename=snownlp-0.12.3-cp36-none-any.whl size=37760959 sha256=3a8aeb4d7482b5bfe44cd3a9aa1334a10b95c9670699d6aaac858a89f68a8277\n",
            "  Stored in directory: /root/.cache/pip/wheels/f3/81/25/7c197493bd7daf177016f1a951c5c3a53b1c7e9339fd11ec8f\n",
            "Successfully built snownlp\n",
            "Installing collected packages: snownlp\n",
            "Successfully installed snownlp-0.12.3\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement stopwords-zh (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for stopwords-zh\u001b[0m\n",
            "Collecting baidu-aip\n",
            "  Downloading https://files.pythonhosted.org/packages/bf/de/0e770c421bd70b0b59d59d1bcf70139cf0ad4263102a7fc2973c6187174a/baidu-aip-2.2.18.0.tar.gz\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from baidu-aip) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->baidu-aip) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->baidu-aip) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->baidu-aip) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->baidu-aip) (3.0.4)\n",
            "Building wheels for collected packages: baidu-aip\n",
            "  Building wheel for baidu-aip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for baidu-aip: filename=baidu_aip-2.2.18.0-cp36-none-any.whl size=15655 sha256=cd73974139d51bf9f67665a7cb346244df1705d69ccde00134f7d3a792049bde\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/f3/20/9567d96b1140f13546bb3e059827cba0d575e213e8ee87f5ea\n",
            "Successfully built baidu-aip\n",
            "Installing collected packages: baidu-aip\n",
            "Successfully installed baidu-aip-2.2.18.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiqPAKs__JKV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3bbdbc5-ca12-4e43-d66f-4210c8e70a77"
      },
      "source": [
        "import csv\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "from statistics import mean\n",
        "\n",
        "from snownlp import SnowNLP \n",
        "from aip import AipNlp\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "import codecs\n",
        "import jieba\n",
        "import jieba.analyse\n",
        "import jieba.posseg as pseg\n",
        "\n",
        "#language processing\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import html\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyxyhJp1Pjks"
      },
      "source": [
        "#Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aiq39nBqzuyE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d224df9-bd3c-411b-98c5-a51ae67cc43a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UL3KgNv506pG"
      },
      "source": [
        "StockNumber = '11_002057_中钢天源'\n",
        "data = pd.read_csv('/content/drive/MyDrive/SZ data Nov 14/11_002057_中钢天源.csv')\n",
        "data['id'] = StockNumber #file name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8HT8luC_iMP"
      },
      "source": [
        "#remove stopwords\n",
        "stopwords = codecs.open('/content/drive/MyDrive/SZ data Nov 14/stopwords-zh.txt', 'r', 'utf-8').read().split('\\n')\n",
        "stopwordsList = ['分页']\n",
        "# p = ['【','*','】','丨','(',')','，','%','-','”','“','、','）','（','：','；',',',':','/','+','=','\\\\',']','[','√','□','—','。',]\n",
        "stopwords.extend(stopwordsList)\n",
        "def removeStop(x):\n",
        "  segList = SnowNLP(x).words\n",
        "  # segList =[seg for seg in segList if seg not in stopwords]\n",
        "  c = ''.join([s for s in segList if s.isalnum() and (s not in stopwords)])\n",
        "  # print(segList)\n",
        "  return c\n",
        "\n",
        "def removeHtml(txt):\n",
        "  # txt = re.sub(r'\\\\x(?=\\w+)\\w+',\"\",txt ) #anything after \\\\x \n",
        "  txt = re.sub(r'\\S+page\\S+',\"\",txt) \n",
        "  # for s in p:\n",
        "  #   txt = re.sub('\\\\'+s, '',txt) #\\[ match special characters\n",
        "\n",
        "  # remove html tags\n",
        "  soup = BeautifulSoup(txt[2:], \"html.parser\")\n",
        "  txt = ''.join(soup.findAll(text=True))\n",
        "  #Replace the HTML character codes with ASCII equvivalent  \n",
        "  txt = html.unescape(txt)\n",
        "\n",
        "  txt = re.sub(r\"http\\S+\", \"\", txt) \n",
        "  txt = txt.strip()\n",
        "\n",
        "  return txt\n",
        "\n",
        "Data = data.copy()\n",
        "Data['标题'] = Data['标题'].apply(lambda x: removeHtml(x))\n",
        "Data['cfontnormal'] = Data['cfontnormal'].apply(lambda x: removeHtml(x))\n",
        "Data['正文'] = Data.loc[Data['正文'].notnull(),'正文'].apply(lambda x: removeHtml(x))\n",
        "\n",
        "Data = Data.replace('',np.NaN)\n",
        "\n",
        "Data['标题'] = Data['标题'].apply(lambda x: removeStop(x))\n",
        "Data['cfontnormal'] = Data['cfontnormal'].apply(lambda x: removeStop(x))\n",
        "Data['正文'] = Data.loc[Data['正文'].notnull(),'正文'].apply(lambda x: removeStop(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxUFfuqSEQ7U"
      },
      "source": [
        "dataSnow = Data.copy()\n",
        "def sentiSnow(x):\n",
        "  senti = SnowNLP(x).sentiments\n",
        "  if senti>0.5:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "dataSnow['标题'] = dataSnow['标题'].apply(lambda x: sentiSnow(x))\n",
        "dataSnow['cfontnormal'] = dataSnow['cfontnormal'].apply(lambda x: sentiSnow(x))\n",
        "dataSnow['正文'] = dataSnow.loc[(dataSnow['正文'].notnull())&(dataSnow['正文']!=''),'正文'].apply(lambda x: sentiSnow(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZs8-Ab7F_EB"
      },
      "source": [
        "APP_ID = '22766485'\n",
        "API_KEY = 'pTkeXFTmQSrVua41zcXTu0qi'\n",
        "SECRET_KEY = 'dEFLyhVDQPcX63mDWeYHDpRm2xyVh0NX'\n",
        "\n",
        "Appid = '22992872'\n",
        "API_Key = 'q04yxeMPkqbTSR4mlgmSvx41'\n",
        "Secret_Key = 'fUM3p1W89nEYBdcKPi6hWrCxV8QGr0kN'\n",
        "\n",
        "client = AipNlp(Appid, API_Key, Secret_Key)\n",
        "\n",
        "#maximal length for baidu api to process is 1000 words\n",
        "# transform language into sentiment\n",
        "dataAip = Data.copy()\n",
        "# dataAip['标题'] = dataAip['标题'].apply(lambda x: client.sentimentClassify(x)['items'][0]['sentiment'])\n",
        "# dataAip['cfontnormal'] = dataAip['cfontnormal'].apply(lambda x: client.sentimentClassify(x)['items'][0]['sentiment'])\n",
        "\n",
        "def sentiCal(x,length):\n",
        "  errorList = []\n",
        "  try: \n",
        "    if len(x) < length:\n",
        "      # print(x, client.sentimentClassify(x))\n",
        "      senti = client.sentimentClassify(x)['items'][0]['sentiment']\n",
        "      return senti \n",
        "    else:\n",
        "      n = len(x)//length\n",
        "      remindar = len(x)%length\n",
        "      sentiList = []\n",
        "      for a in range(n):\n",
        "        senti = client.sentimentClassify(x[a:length+a])['items'][0]['sentiment']\n",
        "        sentiList.append(senti)\n",
        "      sentiList.append(client.sentimentClassify(x[-remindar:])['items'][0]['sentiment'])\n",
        "\n",
        "      return sum(sentiList[:-1])*(n*length/len(x)) + sentiList[-1]*(remindar/len(x))\n",
        "  except:\n",
        "    errorList.append(x)\n",
        "\n",
        "dataAip['标题'] = dataAip.loc[dataAip['标题'].notnull(),'标题'].apply(lambda x: sentiCal(x,1000))\n",
        "dataAip['cfontnormal'] = dataAip.loc[dataAip['cfontnormal'].notnull(),'cfontnormal'].apply(lambda x: sentiCal(x,1000))\n",
        "dataAip['正文'] = dataAip.loc[dataAip['正文'].notnull(),'正文'].apply(lambda x: sentiCal(x,1000))\n",
        "\n",
        "dataAip.loc[dataAip['标题']>=1,'标题']=1\n",
        "dataAip.loc[dataAip['标题']<1,'标题']=0\n",
        "dataAip.loc[dataAip['cfontnormal']>=1,'cfontnormal']=1\n",
        "dataAip.loc[dataAip['cfontnormal']<1,'cfontnormal']=0\n",
        "dataAip.loc[dataAip['正文']>=1,'正文']=1\n",
        "dataAip.loc[dataAip['正文']<1,'正文']=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7Po1nqIHsnI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dc97423-b928-4954-b479-022ed2417683"
      },
      "source": [
        "jieba.enable_paddle()# 启动paddle模式。 0.40版之后开始支持，早期版本不支持\n",
        "\n",
        "def getTopK(df, k1=30, k2=10,k3=50,col='正文'):\n",
        "  counterDict = {}\n",
        "  counterDictGram = {}\n",
        "  Grammar = {}\n",
        "  for Company in Data['id'].unique(): #标题1 will be replaced by identifier stock number\n",
        "    mainText = df[(df['id']==Company)&(df[col].notnull())][col].values\n",
        "    # counter = Counter()\n",
        "    counter = []\n",
        "    grammar = {}\n",
        "    for m in mainText:\n",
        "      for word,tag in pseg.cut(m):#jieba.cut(m,use_paddle=True)\n",
        "        # print(word)\n",
        "        counter.append(word)\n",
        "        # counter.update(word)\n",
        "        if tag not in grammar: \n",
        "          grammar[tag] = [word]\n",
        "        else:\n",
        "          grammar[tag].append(word)\n",
        "        \n",
        "        if tag not in Grammar:\n",
        "          Grammar[tag] = [word]\n",
        "        else:\n",
        "          Grammar[tag].append(word)\n",
        "    \n",
        "    for k,v in grammar.items():\n",
        "      grammar[k] = Counter(grammar[k]).most_common(k2)\n",
        "    for k,v in Grammar.items():\n",
        "      Grammar[k] = Counter(Grammar[k]).most_common(k2)\n",
        "    counterDict[Company] = Counter(counter).most_common(k1)\n",
        "    counterDictGram[Company] = grammar\n",
        "\n",
        "  return counterDict, counterDictGram, Grammar"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Paddle enabled successfully......\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKfVFvU-NcjJ"
      },
      "source": [
        "#This function is to which terms occur the most frequently in the articles \n",
        "def getTopKF(df1, df2, col='正文',k1=50, k2=20): #df1=Data, df2=dataSnow/dataAip\n",
        "  mainText = df1[col].values\n",
        "\n",
        "  positiveCounter = []\n",
        "  negativeCounter = []\n",
        "  companyCounter = {} #for each company, create a dictionary\n",
        "\n",
        "  for i in range(len(mainText)):\n",
        "    if df2[col][i] == 1:\n",
        "      # print(mainText[i])\n",
        "      # print(i)\n",
        "      positiveCounter.extend([word for word,tag in pseg.cut(mainText[i])])\n",
        "    elif df2[col][i] == 0:\n",
        "      negativeCounter.extend([word for word,tag in pseg.cut(mainText[i])])\n",
        "  \n",
        "  topkPositive = Counter(positiveCounter).most_common(k1)\n",
        "  topkNegative = Counter(negativeCounter).most_common(k1)\n",
        "\n",
        "  for Company in df1['id'].unique():\n",
        "    mainText2 = df1[df1['id']==Company][col].values\n",
        "    senti = {}\n",
        "    senti['positive'] = []\n",
        "    senti['negative'] = []\n",
        "    for i in range(len(mainText2)):\n",
        "      # print(df2[df2['id']==Company][col][i])\n",
        "      if df2[df2['id']==Company][col].values[i] == 1:\n",
        "        senti['positive'].extend([word for word,tag in pseg.cut(mainText2[i])])\n",
        "      elif df2[df2['id']==Company][col].values[i] == 0:\n",
        "        senti['negative'].extend([word for word,tag in pseg.cut(mainText2[i])])\n",
        "    \n",
        "    senti['positive'] = Counter(senti['positive']).most_common(k2)\n",
        "    senti['negative'] = Counter(senti['negative']).most_common(k2)\n",
        "    companyCounter[Company] = senti\n",
        "    \n",
        "  return topkPositive, topkNegative, companyCounter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pqiJA54OD9A"
      },
      "source": [
        "def dataFrameTransformation(counterDic, df1, df2, company,col='正文'): #df1 = Data, df2=dataSnow/dataAip\n",
        "  #Find out if a particular review has the word from topk list\n",
        "  freqReview = []\n",
        "  reviews = df1[col].values\n",
        "  for i in range(len(reviews)):\n",
        "    # tempCounter = Counter([word for word,tag in pseg.cut(reviews[i]) if (df[col].values[i] = 1) or (df[col].values[i] = 0)])\n",
        "    topkinReview = [1 if ((df2[col].values[i]==0) or  (df2[col].values[i]==1)) and (word in reviews[i])\\\n",
        "                    else 0 for (word, wordcount) in counterDic[company]]\n",
        "    freqReview.append(topkinReview)\n",
        "\n",
        "  freqReviewDf = pd.DataFrame(freqReview)\n",
        "  dfName = []\n",
        "  for c in counterDic[company]:\n",
        "    dfName.append(c[0])\n",
        "  freqReviewDf.columns = dfName\n",
        "\n",
        "  finaldf = df2[[col,'id']].join(freqReviewDf)\n",
        "\n",
        "  return freqReviewDf, finaldf\n",
        "\n",
        "# Simple example of getting pairwise mutual information of a term\n",
        "def pmiCal(df, x, col='正文'):\n",
        "    pmilist=[]\n",
        "    for i in [1,0]: #positive, negativ\n",
        "        for j in [0,1]: #the word in the review or not\n",
        "            px = sum(df[col]==i)/len(df)  #context: number of 'positive'/ all rows \n",
        "            py = sum(df[x]==j)/len(df) #word: number of reviews that conatain the word/ all rows \n",
        "            pxy = len(df[(df[col]==i) & (df[x]==j)])/len(df)\n",
        "            # print(px,py,x)\n",
        "            if pxy==0:#Log 0 cannot happen\n",
        "                pmi = math.log((pxy+0.0001)/(px*py))\n",
        "            else:\n",
        "                pmi = math.log(pxy/(px*py))\n",
        "            pmilist.append([i]+[j]+[px]+[py]+[pxy]+[pmi])\n",
        "    pmidf = pd.DataFrame(pmilist)\n",
        "    pmidf.columns = ['x','y','px','py','pxy','pmi']\n",
        "    return pmidf\n",
        "\n",
        "def pmiIndivCal(df,term,gt, col='正文'):\n",
        "    px = sum(df[col]==gt)/len(df) #context: number of 'positive'/ all rows \n",
        "    py = sum(df[term]==1)/len(df) #word: number of reviews that conatain the word/ all rows \n",
        "    pxy = len(df[(df[col]==gt) & (df[term]==1)])/len(df)\n",
        "    if pxy==0:#Log 0 cannot happen\n",
        "        pmi = math.log((pxy+0.0001)/(px*py))\n",
        "    else:\n",
        "        pmi = math.log(pxy/(px*py))\n",
        "    return pmi\n",
        "\n",
        "def pmi(finaldf, counterDic, company, col='正文'):\n",
        "    pmilist = []\n",
        "    pmiposlist = []\n",
        "    pmineglist = []\n",
        "    for word in counterDic[company]:\n",
        "        pmilist.append([word[0]]+[pmiCal(finaldf,word[0])]) #k *([the word, 'x','y','px','py','pxy','pmi'])\n",
        "        pmiposlist.append([word[0]]+[pmiIndivCal(finaldf,word[0],1,col)]) #positive #k*([the word, pmi])\n",
        "        pmineglist.append([word[0]]+[pmiIndivCal(finaldf,word[0],0,col)]) #negative #k*([the word, pmi])\n",
        "    pmidf = pd.DataFrame(pmilist)\n",
        "    pmiposlist = pd.DataFrame(pmiposlist)\n",
        "    pmineglist = pd.DataFrame(pmineglist)\n",
        "    pmiposlist.columns = ['word','pmi']\n",
        "    pmineglist.columns = ['word','pmi']\n",
        "    pmidf.columns = ['word','pmi']\n",
        "    return pmiposlist, pmineglist, pmidf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qUpexcxPIRt"
      },
      "source": [
        "def split(x):\n",
        "  x = ' '.join([word for word,tag in pseg.cut(x)])\n",
        "  return x \n",
        "\n",
        "def tfidf(Data, TermList, company, col='正文'):\n",
        "  vocabulary = list(TermList['word'])\n",
        "  vectorizer = TfidfVectorizer(vocabulary=vocabulary, use_idf=True, smooth_idf=True, norm=None)\n",
        "  Data = Data[(Data['id']==company)&(Data[col].notnull())]\n",
        "  Data[col] = Data[col].apply(split)\n",
        "  arrayTFIDF = vectorizer.fit_transform(Data[col]).toarray()\n",
        "  dfTFIDF = pd.DataFrame(arrayTFIDF,columns=vocabulary)\n",
        "\n",
        "  return dfTFIDF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rSSkyngOI3l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9974310-15ee-4921-9119-9f76cae67bdc"
      },
      "source": [
        "#Extract the most frequently mentioned terms and understand its correlation with sentiments\n",
        "\n",
        "counterDic, counterDicGram, Grammar = getTopK(Data, k1=100)\n",
        "\n",
        "#SnowNLP\n",
        "# topkPositiveSnow, topkNegativeSnow, sentiCompanySnow = getTopKF(Data,dataSnow)\n",
        "#Baidu Aip\n",
        "topkPositiveB, topkNegativeB, sentiCompanyB = getTopKF(Data,dataAip)\n",
        "\n",
        "#SnowNLP\n",
        "# freqReviewDfSnow, finaldfSnow = dataFrameTransformation(counterDic, Data, dataSnow, StockNumber ,col='正文')\n",
        "# pmiposlistSnow, pmineglistSnow, pmidfPosiSnow = pmi(finaldfSnow, counterDic, StockNumber)\n",
        "\n",
        "#Baidu Aip\n",
        "freqReviewDfAip, finaldfAip = dataFrameTransformation(counterDic, Data, dataAip, StockNumber ,col='正文')\n",
        "pmiposlistAip, pmineglistAip, pmidfPosiAip = pmi(finaldfAip, counterDic, StockNumber)\n",
        "\n",
        "#SnowNLP Positive/Negative terms \n",
        "# PosTermSnow = pmiposlistSnow.sort_values('pmi',ascending=0).head(50)\n",
        "# NegTermSnow = pmineglistSnow.sort_values('pmi',ascending=False).head(50)\n",
        "#BaiduAip\n",
        "PosTermAip = pmiposlistAip.sort_values('pmi',ascending=0).head(50)\n",
        "NegTermAip = pmineglistAip.sort_values('pmi',ascending=False).head(50)\n",
        "\n",
        "#SnowNLP TFIDF\n",
        "# dfTFIDF_Pos_Snow = tfidf(Data, PosTermSnow, StockNumber,'正文')\n",
        "# dfTFIDF_Neg_Snow = tfidf(Data, NegTermSnow, StockNumber,'正文')\n",
        "# BaiduAip\n",
        "dfTFIDF_Pos_Aip = tfidf(Data, PosTermAip, StockNumber,'正文')\n",
        "dfTFIDF_Neg_Aip = tfidf(Data, NegTermAip, StockNumber,'正文')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2T9nrHmMPmPf"
      },
      "source": [
        "#Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgkQC00lPpES"
      },
      "source": [
        "##Differences between SnowNLP and Baidu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI5Ol83bUpa6"
      },
      "source": [
        "#Sentiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Syxn-bSZUlRa"
      },
      "source": [
        "# filenameSnow = '/content/drive/My Drive/MarkThesis/Sentiment/'+ StockNumber+'SnowNLP_Sentiment.csv'\n",
        "# dataSnow.to_csv(filenameSnow)\n",
        "\n",
        "# filenameBaidu = '/content/drive/My Drive/MarkThesis/Sentiment/'+ StockNumber+'SnowNLP_Sentiment.csv'\n",
        "# dataAip.to_csv(filenameBaidu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vw_sUy2rWh9A"
      },
      "source": [
        "# filenameTFIDF_P_S = '/content/drive/My Drive/MarkThesis/TFIDF/'+ StockNumber+'_Positive_SnowNLP.csv'\n",
        "# dfTFIDF_Pos_Snow.to_csv(filenameTFIDF_P_S )\n",
        "\n",
        "# filenameTFIDF_N_S = '/content/drive/My Drive/MarkThesis/TFIDF/'+ StockNumber+'_Negative_SnowNLP.csv'\n",
        "# dfTFIDF_Neg_Snow.to_csv(filenameTFIDF_N_S)\n",
        "\n",
        "\n",
        "filenameTFIDF_P_B = '/content/drive/My Drive/MarkThesis/TFIDF_new/'+ StockNumber+'_Positive_BaiduAip.csv'\n",
        "dfTFIDF_Pos_Aip.to_csv(filenameTFIDF_P_B)\n",
        "\n",
        "filenameTFIDF_N_B = '/content/drive/My Drive/MarkThesis/TFIDF_new/'+ StockNumber+'_Negative_BaiduAip.csv'\n",
        "dfTFIDF_Neg_Aip.to_csv(filenameTFIDF_N_B)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9ruhQC_bQkF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}